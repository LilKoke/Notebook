{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* アンサンブル（ensemble):予測器のグループ\n* アンサンブル学習（ensemble learning):一群の予測器のよそくを1つにまとめる学習\n* アンサンブルメソッド（ensemble method):アンサンブル学習アルゴリズム\n* ランダムフォレスト（random forest):訓練セットかr無作為に作った様々なサブセットを使って一連の決定木分類器を訓練し、予測するときにはすべての木の予測を集め、多数決で全体の予測クラスを決める\n* ハード投票（hard voting)分類器：各分類器の予測を集め、多数決で決まったクラスを全体の予測とする\n* 弱学習器(weak learner: 無作為な推測よりもわずか程度に良い）を十分集め、それぞれが十分多種多様なら、アンサンブルは強学習器（strong learner:高い正解率を達成する）になる\n* 大数の法則（law of large numbers)：すべての分類器が完全に独立していて、誤りに相関関係がない場合に成り立つ","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import make_moons\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_moons(n_samples=10000, noise=0.25)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) \n\nlog_clf = LogisticRegression()\nrnd_clf = RandomForestClassifier()\nsvm_clf = SVC(probability = True)\n\nvoting_clf = VotingClassifier(\n    estimators = [('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n    voting='hard')\nvoting_clf.fit(X_train, y_train)\n\nfor clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-08-12T09:19:19.755881Z","iopub.execute_input":"2021-08-12T09:19:19.756366Z","iopub.status.idle":"2021-08-12T09:19:28.083427Z","shell.execute_reply.started":"2021-08-12T09:19:19.756314Z","shell.execute_reply":"2021-08-12T09:19:28.082174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nheads_proba = 0.51\ncoin_tosses = (np.random.rand(10000,10)< heads_proba).astype(np.int32)#51%の確率で1となるようにする\ncumulative_heads_ratio = np.cumsum(coin_tosses, axis=0)/np.arange(1,10001).reshape(-1,1)#毎回どれくらいの割合表だったかを計算\nplt.figure(figsize=(8,3.5))\nplt.plot(cumulative_heads_ratio)\nplt.plot([0,10000], [0.51,0.51], \"k--\", linewidth=2, label=\"51%\")\nplt.plot([0,10000], [0.5,0.5], \"k-\", linewidth=2, label=\"50%\")\nplt.xlabel(\"Number of coin tosses\")\nplt.ylabel(\"Heads ratio\")\nplt.legend(loc=\"lower right\")\nplt.axis([0, 10000, 0.42, 0.58])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-12T09:19:28.08483Z","iopub.execute_input":"2021-08-12T09:19:28.085096Z","iopub.status.idle":"2021-08-12T09:19:28.321789Z","shell.execute_reply.started":"2021-08-12T09:19:28.08507Z","shell.execute_reply":"2021-08-12T09:19:28.320748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* ソフト投票(soft voting)：すべての分類器がクラスに属する確率を推計できるメソッドを持つ場合、個別の分類器が推計する確率を平均し、もっとも確率の高いクラスを予測クラスとして返す\n\n訓練セットから無作為に別々のサブセットをサンプリングして訓練する\n* バギング（bagging: bootstrap aggreating)：サンプリングが重複あり\n* ペースティング（pasting):サンプリングが重複なし\nすべての分類器を予測したら、アンサンブルは単純にすべての予測器の予測を集計して新インスタンスに対する予測をする。\\\n集計関数\n* 統計モード（statistical mode：予測の最頻値をとる）→分類\n* 平均→回帰\n\n異なるCPUコアや異なるサーバーを使ってすべての分類器を並列に訓練でき、予測も並列に実行できる！\n\n個々の予測器が訓練に使うサブセットの多様性はバギングのほうがペースティングより高い→バイアスが高くなる\\\nいっぽうで、予測器の相関が下がる→アンサンブルの分散は下がる\\\nバギングのほうがペースティングより良いモデルになることが多い","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nbag_clf=BaggingClassifier(\n    DecisionTreeClassifier(), n_estimators = 500, \n    max_samples=100, bootstrap=True, n_jobs=-1)\nbag_clf.fit(X_train, y_train)\ny_pred=bag_clf.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T09:19:28.32395Z","iopub.execute_input":"2021-08-12T09:19:28.324566Z","iopub.status.idle":"2021-08-12T09:19:31.060321Z","shell.execute_reply.started":"2021-08-12T09:19:28.324513Z","shell.execute_reply":"2021-08-12T09:19:31.059237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* OOB（out-of-bag)検証：訓練中に見られなかったインスタンスを使って検証すること\n* ランダムパッチメソッド(random patch)：訓練インスタンスと特徴量の両方をサンプリングすること\n* ランダムサブスペースメソッド（random subspace)：訓練インスタンスはすべて使い、特徴量だけサンプリングすること\\\n特徴量をサンプリングすると、予測器の多様性は上がり、わずかにバイアスが上がる分、分散を小さくすることができる。","metadata":{}},{"cell_type":"code","source":"bag_clf = BaggingClassifier(\n    DecisionTreeClassifier(), n_estimators = 500,\n    bootstrap = True, n_jobs = -1, oob_score=True\n)\nbag_clf.fit(X_train, y_train)\nbag_clf.oob_score_","metadata":{"execution":{"iopub.status.busy":"2021-08-12T09:19:31.062031Z","iopub.execute_input":"2021-08-12T09:19:31.062361Z","iopub.status.idle":"2021-08-12T09:19:34.18334Z","shell.execute_reply.started":"2021-08-12T09:19:31.062324Z","shell.execute_reply":"2021-08-12T09:19:34.182354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\ny_pred = bag_clf.predict(X_test)\naccuracy_score(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T09:19:34.184551Z","iopub.execute_input":"2021-08-12T09:19:34.184878Z","iopub.status.idle":"2021-08-12T09:19:34.561271Z","shell.execute_reply.started":"2021-08-12T09:19:34.184848Z","shell.execute_reply":"2021-08-12T09:19:34.560528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\nrnd_clf.fit(X_train, y_train)\n\ny_pred_rf = rnd_clf.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T09:19:34.562404Z","iopub.execute_input":"2021-08-12T09:19:34.562715Z","iopub.status.idle":"2021-08-12T09:19:36.582943Z","shell.execute_reply.started":"2021-08-12T09:19:34.562683Z","shell.execute_reply":"2021-08-12T09:19:36.581967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ランダムフォレスト：決定木のアンサンブル\\\nバギングメソッド(たまにペースティング）で訓練される\\\nmax_samples=訓練セットサイズ\nノードを分割するときに最良の特徴量を探すのではなく、特徴量の無作為なサブセットから最良の特徴量を探す\\\n","metadata":{}},{"cell_type":"code","source":"#random forestの言いかえ\nbag_clf = BaggingClassifier(\n    DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\n    n_estimators  = 500, max_samples = 1.0, bootstrap = True, n_jobs = -1#ペースティングを使いたい場合はbootstrapをfalseに\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T09:19:36.583889Z","iopub.execute_input":"2021-08-12T09:19:36.584148Z","iopub.status.idle":"2021-08-12T09:19:36.589497Z","shell.execute_reply.started":"2021-08-12T09:19:36.584122Z","shell.execute_reply":"2021-08-12T09:19:36.588233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Extra-Tree:Extremely Randomized Trees\\\n最良のしきい値を探すのではなく、個々の特徴量の閾値を無作為なものにする\n\n普段の流れ：\n* まず閾値の候補として、データの間の数を選ぶ\n* それぞれの候補に関してジニ係数などを求めて、最適な閾値を選ぶ\n","metadata":{}},{"cell_type":"markdown","source":"その特徴量を使うノードが平均して不純度をどれくらい減らすかを調べることにより、特徴量の重要性を測る。","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\niris = load_iris()\nrnd_clf = RandomForestClassifier(n_estimators = 500, n_jobs = -1)#n_jobs 訓練、予測に使うCPUコアの数を指示\nrnd_clf.fit(iris[\"data\"], iris[\"target\"])","metadata":{"execution":{"iopub.status.busy":"2021-08-12T09:19:36.590956Z","iopub.execute_input":"2021-08-12T09:19:36.59134Z","iopub.status.idle":"2021-08-12T09:19:37.619954Z","shell.execute_reply.started":"2021-08-12T09:19:36.591308Z","shell.execute_reply":"2021-08-12T09:19:37.618872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n    print(name, score)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T09:19:37.62211Z","iopub.execute_input":"2021-08-12T09:19:37.622387Z","iopub.status.idle":"2021-08-12T09:19:37.732044Z","shell.execute_reply.started":"2021-08-12T09:19:37.622358Z","shell.execute_reply":"2021-08-12T09:19:37.731307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ブースティング(boosting):複数の弱学習器を結合して強学習器を作れるあらゆるアンサンブルメソッド\n* AdaBoost\n* 勾配ブースティング（gradient boosting)\n\nAdaBoostの手順\n* ベースの分類器を訓練し、訓練セットを対象として予測\n* 分類に失敗した訓練真スタンスの相対的な重みを上げる\n* 更新された重みを使って次の分類器を訓練し、訓練セットの予測をして、重みを更新\\\nこれを繰り返し\\\n➡コスト関数を最小化するためにパラメータを操作するのではなく、予測器を追加してアンサンブルを改良していく\nhttps://www.youtube.com/watch?v=LsK-xG1cLYA\n1. 個々のインスタンスに駆けられた重み$w^{(i)}$は初期状態で$\\frac{1}{m}$\n2. 誤り率$r_1$が計算される$r_j = \\frac{\\Sigma_{i=1, \\hat{y}_j^{(i)}\\neq y^{(i)}}^{m}w^{(i)}}{\\Sigma_{i=1}^m w^{(i)}}$\n3. 予測器の重みが計算される➡正確なほど重みは高くなる$\\alpha_j = \\eta \\log \\frac{1-r_j}{r_j}$\n4. インスタンスの重みを更新する。➡誤った分類をされたインスタンスの重みは大きくなる$for\\:i = 1,2,...,m\\:w^{(i)} = \\left\\{ \\,\n    \\begin{aligned}\n    & w^{(i)}　　　　　if  \\hat{y}_j^{(i)}=y^{(i)}\\\\\n    & w^{(i)}exp(\\alpha_j)　if  \\hat{y}_j^{(i)}\\neq y^{(i)}\n    \\end{aligned}\n\\right.$\n5. すべてのインスタンスの重みを正規化\n6. すべての予測器の予測を計算し、予測器の重み$\\alpha_j$を使って予測に重みを与えたうえで、重み付きの多数決で選ばれたクラスを予測結果とする。\n\n決定株(decision stump):max_depth=1の決定木","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\nX, y = make_moons(n_samples=10000, noise=0.25)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) \nada_clf = AdaBoostClassifier(\n    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n    algorithm=\"SAMME.R\", learning_rate = 0.5\n)\nada_clf.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T09:19:37.733232Z","iopub.execute_input":"2021-08-12T09:19:37.733628Z","iopub.status.idle":"2021-08-12T09:19:38.919557Z","shell.execute_reply.started":"2021-08-12T09:19:37.73357Z","shell.execute_reply":"2021-08-12T09:19:38.918405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"勾配ブースティング:新予測器を前の予測器の残差（residual error)に適合させようとする。\\\nhttps://www.youtube.com/watch?v=3CC4N4z3GJc \\\n収縮(shrinkage)：learning_rateハイパーパラメータは個々の木の影響力を調整するが、これを低い値にすることによって、多くの決定木を追加しなけらばならないが、予測の汎化性能が上がる\\\n決定木の最適な数は、早期打ち切りを使えばわかる。\n","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg1 = DecisionTreeRegressor(max_depth=2)\ntree_reg1.fit(X,y)\n\ny2 = y - tree_reg1.predict(X)\ntree_reg2 = DecisionTreeRegressor(max_depth=2)\ntree_reg2.fit(X,y2)\n\ny3 = y2 - tree_reg2.predict(X)\ntree_reg3 = DecisionTreeRegressor(max_depth=2)\ntree_reg3.fit(X,y3)\n\ny_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\n\ngbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\ngbrt.fit(X,y)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T09:19:47.544794Z","iopub.execute_input":"2021-08-12T09:19:47.545116Z","iopub.status.idle":"2021-08-12T09:19:47.57245Z","shell.execute_reply.started":"2021-08-12T09:19:47.545087Z","shell.execute_reply":"2021-08-12T09:19:47.571466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nX_train, X_val, y_train, y_val = train_test_split(X,y)\n\ngbrt=GradientBoostingRegressor(max_depth=2, n_estimators = 120)\ngbrt.fit(X_train, y_train)\n\nerrors = [mean_squared_error(y_val, y_pred)\n          for y_pred in gbrt.staged_predict(X_val)]\nbst_n_estimators = np.argmin(errors)+1\n\ngbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators)\ngbrt_best.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T09:25:08.892029Z","iopub.execute_input":"2021-08-12T09:25:08.892351Z","iopub.status.idle":"2021-08-12T09:25:10.0465Z","shell.execute_reply.started":"2021-08-12T09:25:08.892321Z","shell.execute_reply":"2021-08-12T09:25:10.045316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)#既存の決定木を残す\n\nmin_val_error = float(\"inf\")\nerror_going_up=0\nfor n_estimators in range(1,120):\n    gbrt.n_estimators = n_estimators\n    gbrt.fit(X_train, y_train)\n    y_pred=gbrt.predict(X_val)\n    val_error = mean_squared_error(y_val, y_pred)\n    if val_error < min_val_error:\n        min_val_error = val_error\n        error_going_up=0\n    else:\n        error_going_up += 1\n        if error_going_up == 5:\n            break","metadata":{"execution":{"iopub.status.busy":"2021-08-12T09:46:36.515883Z","iopub.execute_input":"2021-08-12T09:46:36.516219Z","iopub.status.idle":"2021-08-12T09:46:38.174011Z","shell.execute_reply.started":"2021-08-12T09:46:36.516188Z","shell.execute_reply":"2021-08-12T09:46:38.173066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"個々の決定木を訓練するために使われる訓練インスタンスの割合を指定→個々の決定木は無作為に選択された訓練インスタンスうを使って訓練される。\\\n➡バイアスを上げて分散を下げる\\\nこれを確率的勾配ブースティング（stochastic gradient boosting)という","metadata":{}},{"cell_type":"code","source":"import xgboost \n\nxgb_reg = xgboost.XGBRegressor()\nxgb_reg.fit(X_train, y_train)\ny_pred = xgb_reg.predict(X_val)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T09:51:02.500537Z","iopub.execute_input":"2021-08-12T09:51:02.502686Z","iopub.status.idle":"2021-08-12T09:51:02.973625Z","shell.execute_reply.started":"2021-08-12T09:51:02.502636Z","shell.execute_reply":"2021-08-12T09:51:02.972563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_reg.fit(X_train, y_train,\n           eval_set = [(X_val, y_val)], early_stopping_rounds=2)\ny_pred=xgb_reg.predict(X_val)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T09:52:01.217669Z","iopub.execute_input":"2021-08-12T09:52:01.218054Z","iopub.status.idle":"2021-08-12T09:52:01.331399Z","shell.execute_reply.started":"2021-08-12T09:52:01.218018Z","shell.execute_reply":"2021-08-12T09:52:01.330595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"スタッキング(stacking,　スタック汎化：stacked generalization):アンサンブルに含まれるすべての予測器の予測を集計するところもモデルとして訓練しよう！\\\nブレンダ：blender, メタ学習器：meta learner…最後の予測器のこと。最終的な予測を返す。\nシンプルなスタッキング\n1. 訓練セットを２つのサブセットに分割→１つは第１層の予測器の訓練\n2. その予測器で第2セットの予測→この予測値はブレンダの入力\n3. 第2セットの実際の値を訓練データとしてブレンダを訓練\n\n実際は異なるブレンダを訓練して、第2層とする。\n第3層を追加する。","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}